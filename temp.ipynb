{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3fda7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bddf32cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:37:43 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/04/11 09:37:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BD_Lab03_2.1_MLlib\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b9a4c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_path = \"/Users/macos/Desktop/hcmus/big_data/Lab-3-Big-Data/train.csv\"\n",
    "test_path = \"/Users/macos/Desktop/hcmus/big_data/Lab-3-Big-Data/test.csv\"\n",
    "train_data = spark.read.csv(train_path, header=True, inferSchema=True)\n",
    "test_data = spark.read.csv(test_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44fcb6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|    trip_duration|\n",
      "+-------+-----------------+\n",
      "|  count|          1382668|\n",
      "|   mean|729.9852343440363|\n",
      "| stddev|445.7921009969074|\n",
      "|    min|                1|\n",
      "|    max|             2075|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculate Q1 and Q3 for outlier removal\n",
    "quantiles = train_data.approxQuantile(\"trip_duration\", [0.25, 0.75], 0.01)\n",
    "Q1, Q3 = quantiles\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers\n",
    "clean_data = train_data.filter((col(\"trip_duration\") >= lower_bound) & (col(\"trip_duration\") <= upper_bound))\n",
    "train_data = clean_data\n",
    "train_data.select(\"trip_duration\").describe().show()\n",
    "\n",
    "# Prepare features - MLlib uses RDDs of LabeledPoint\n",
    "def parse_data(row):\n",
    "    features = [\n",
    "        row.passenger_count,\n",
    "        row.pickup_longitude,\n",
    "        row.pickup_latitude,\n",
    "        row.dropoff_longitude,\n",
    "        row.dropoff_latitude\n",
    "    ]\n",
    "    return LabeledPoint(row.trip_duration, features)\n",
    "\n",
    "# Convert DataFrames to RDDs\n",
    "train_rdd = train_data.rdd.map(parse_data)\n",
    "test_rdd = test_data.rdd.map(lambda row: (\n",
    "    row.id,\n",
    "    [row.passenger_count, row.pickup_longitude, row.pickup_latitude, \n",
    "     row.dropoff_longitude, row.dropoff_latitude]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f437f747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 09:45:46 WARN DAGScheduler: Broadcasting large task binary with size 1496.7 KiB\n",
      "25/04/11 09:45:47 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/04/11 09:45:48 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/04/11 09:45:49 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/04/11 09:45:50 WARN DAGScheduler: Broadcasting large task binary with size 1069.0 KiB\n",
      "25/04/11 09:45:50 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "25/04/11 09:45:51 WARN DAGScheduler: Broadcasting large task binary with size 1379.4 KiB\n",
      "25/04/11 09:45:52 WARN DAGScheduler: Broadcasting large task binary with size 11.1 MiB\n",
      "25/04/11 09:45:53 WARN DAGScheduler: Broadcasting large task binary with size 1652.8 KiB\n",
      "25/04/11 09:45:54 WARN DAGScheduler: Broadcasting large task binary with size 14.6 MiB\n",
      "25/04/11 09:45:55 WARN DAGScheduler: Broadcasting large task binary with size 1889.6 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split training data\n",
    "train_train_rdd, val_train_rdd = train_rdd.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Decision Tree model\n",
    "model = DecisionTree.trainRegressor(\n",
    "    train_train_rdd,\n",
    "    categoricalFeaturesInfo={},  # Empty dict = all features are continuous\n",
    "    impurity='variance',\n",
    "    maxDepth=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1852dbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 10:01:13 WARN DAGScheduler: Broadcasting large task binary with size 16.1 MiB\n",
      "25/04/11 10:01:53 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 55 (TID 101): Attempting to kill Python Worker\n",
      "25/04/11 10:01:53 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 55 (TID 101): Attempting to kill Python Worker\n",
      "25/04/11 10:02:05 WARN DAGScheduler: Broadcasting large task binary with size 16.1 MiB\n",
      "[Stage 56:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 289.315835\n",
      "R²: 0.366705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Rest of the code remains the same for predictions and evaluation\n",
    "predictions = model.predict(val_train_rdd.map(lambda x: x.features))\n",
    "labels_and_preds = val_train_rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = RegressionMetrics(labels_and_preds)\n",
    "rmse = metrics.rootMeanSquaredError\n",
    "r2 = metrics.r2\n",
    "\n",
    "print(f\"RMSE: {rmse:6f}\")\n",
    "print(f\"R²: {r2:6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26cd21d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 10:43:29 WARN DAGScheduler: Broadcasting large task binary with size 1517.5 KiB\n",
      "25/04/11 10:43:30 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/04/11 10:43:31 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/04/11 10:43:33 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/04/11 10:43:34 WARN DAGScheduler: Broadcasting large task binary with size 1142.0 KiB\n",
      "25/04/11 10:43:34 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "25/04/11 10:43:36 WARN DAGScheduler: Broadcasting large task binary with size 1460.9 KiB\n",
      "25/04/11 10:43:37 WARN DAGScheduler: Broadcasting large task binary with size 11.6 MiB\n",
      "25/04/11 10:43:38 WARN DAGScheduler: Broadcasting large task binary with size 1799.7 KiB\n",
      "25/04/11 10:43:40 WARN DAGScheduler: Broadcasting large task binary with size 15.5 MiB\n",
      "25/04/11 10:43:41 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/04/11 10:43:43 WARN DAGScheduler: Broadcasting large task binary with size 17.2 MiB\n",
      "25/04/11 10:43:55 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 102 (TID 192): Attempting to kill Python Worker\n",
      "25/04/11 10:43:55 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 102 (TID 192): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Pandas >= 1.0.5 must be installed; however, it was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/hcmus/big_data/Lab-3-Big-Data/venv/lib/python3.9/site-packages/pyspark/sql/pandas/utils.py:27\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     have_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m     16\u001b[0m output_df \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mtoDF([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 17\u001b[0m \u001b[43moutput_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults_mllib.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/hcmus/big_data/Lab-3-Big-Data/venv/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:86\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _create_converter_to_pandas\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[0;32m---> 86\u001b[0m \u001b[43mrequire_minimum_pandas_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     90\u001b[0m jconf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_jconf\n",
      "File \u001b[0;32m~/Desktop/hcmus/big_data/Lab-3-Big-Data/venv/lib/python3.9/site-packages/pyspark/sql/pandas/utils.py:34\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     raised_error \u001b[38;5;241m=\u001b[39m error\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m have_pandas:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit was not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m minimum_pandas_version\n\u001b[1;32m     36\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mraised_error\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LooseVersion(pandas\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m LooseVersion(minimum_pandas_version):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour version was \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (minimum_pandas_version, pandas\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m     41\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: Pandas >= 1.0.5 must be installed; however, it was not found."
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Assuming train_rdd and test_rdd are already prepared\n",
    "model = DecisionTree.trainRegressor(train_rdd, categoricalFeaturesInfo={}, impurity='variance', maxDepth=20)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_features = test_rdd.map(lambda x: x[1])\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "# Get IDs and combine with predictions\n",
    "test_ids = test_rdd.map(lambda x: x[0])\n",
    "output = test_ids.zip(predictions.map(float))  # Convert to native float\n",
    "\n",
    "# Save results\n",
    "output_df = output.toDF([\"id\", \"prediction\"])\n",
    "output_df.toPandas().to_csv(\"results_mllib.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "546a0e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 10:48:38 WARN DAGScheduler: Broadcasting large task binary with size 17.2 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_df.toPandas().to_csv(\"results_mllib.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e9014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
