{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9da7f8",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4961174",
   "metadata": {},
   "source": [
    "### Create session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "524e6df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/11 23:37:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLlib\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b7facd",
   "metadata": {},
   "source": [
    "### Prepare data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8ab8f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_path = \"/Users/macos/Desktop/hcmus/big_data/Lab-3-Big-Data/train.csv\"\n",
    "test_path = \"/Users/macos/Desktop/hcmus/big_data/Lab-3-Big-Data/test.csv\"\n",
    "train_df = spark.read.csv(train_path, header=True, inferSchema=True)\n",
    "test_df = spark.read.csv(test_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5262bef",
   "metadata": {},
   "source": [
    "#### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf47c4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "quantiles = train_df.approxQuantile(\"trip_duration\", [0.25, 0.75], 0.01)\n",
    "Q1, Q3 = quantiles\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "train_df = train_df.filter((col(\"trip_duration\") >= lower_bound) & (col(\"trip_duration\") <= upper_bound))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55036a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "\n",
    "def to_labeled_point(row):\n",
    "    features = [row[c] for c in feature_cols]\n",
    "    return LabeledPoint(row[\"trip_duration\"], Vectors.dense(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75b61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = train_df.rdd.map(to_labeled_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66771d0",
   "metadata": {},
   "source": [
    "### Split data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6301eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_rdd.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d5849",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2157050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/12 03:44:35 WARN DAGScheduler: Broadcasting large task binary with size 1496.8 KiB\n",
      "25/04/12 03:44:35 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/04/12 03:44:36 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/04/12 03:44:37 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/04/12 03:44:38 WARN DAGScheduler: Broadcasting large task binary with size 1069.0 KiB\n",
      "25/04/12 03:44:39 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "25/04/12 03:44:39 WARN DAGScheduler: Broadcasting large task binary with size 1379.4 KiB\n",
      "25/04/12 03:44:40 WARN DAGScheduler: Broadcasting large task binary with size 11.1 MiB\n",
      "25/04/12 03:44:41 WARN DAGScheduler: Broadcasting large task binary with size 1652.8 KiB\n",
      "25/04/12 03:44:42 WARN DAGScheduler: Broadcasting large task binary with size 14.6 MiB\n",
      "25/04/12 03:44:44 WARN DAGScheduler: Broadcasting large task binary with size 1889.6 KiB\n",
      "25/04/12 03:44:45 WARN DAGScheduler: Broadcasting large task binary with size 18.6 MiB\n",
      "25/04/12 03:44:46 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "25/04/12 03:44:48 WARN DAGScheduler: Broadcasting large task binary with size 22.9 MiB\n",
      "25/04/12 03:44:49 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/04/12 03:44:51 WARN DAGScheduler: Broadcasting large task binary with size 27.5 MiB\n",
      "25/04/12 03:44:52 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/04/12 03:44:54 WARN DAGScheduler: Broadcasting large task binary with size 32.7 MiB\n",
      "25/04/12 03:44:56 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/04/12 03:44:58 WARN DAGScheduler: Broadcasting large task binary with size 38.4 MiB\n",
      "25/04/12 03:45:01 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = DecisionTree.trainRegressor(\n",
    "    train_data,\n",
    "    categoricalFeaturesInfo={},\n",
    "    impurity=\"variance\",\n",
    "    maxDepth=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf511d",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance_from_debug_string(debug_string, choosen_header):\n",
    "    feature_counts = defaultdict(int)\n",
    "    \n",
    "    pattern = re.compile(r\"If \\(feature (\\d+) <=.*\\)\")\n",
    "    \n",
    "    for line in debug_string.split(\"\\n\"):\n",
    "        match = pattern.search(line)\n",
    "        if match:\n",
    "            feature_index = int(match.group(1))\n",
    "            feature_counts[feature_index] += 1\n",
    "    \n",
    "    total_importance = sum(feature_counts.values())\n",
    "    feature_importances = {choosen_header[i]: (count / total_importance if total_importance > 0 else 0.0)\n",
    "                          for i, count in feature_counts.items()}\n",
    "    \n",
    "    print(\"\\nFeature Importances:\")\n",
    "    for feature, importance in feature_importances.items():\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "    \n",
    "    return feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac804d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances:\n",
      "dropoff_longitude: 0.1956\n",
      "dropoff_latitude: 0.2399\n",
      "passenger_count: 0.1215\n",
      "pickup_longitude: 0.1471\n",
      "pickup_latitude: 0.2959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dropoff_longitude': 0.19557646397977813,\n",
       " 'dropoff_latitude': 0.23992416795393906,\n",
       " 'passenger_count': 0.12151383232692038,\n",
       " 'pickup_longitude': 0.1470720404437579,\n",
       " 'pickup_latitude': 0.29591349529560457}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_str = model.toDebugString()\n",
    "get_feature_importance_from_debug_string(debug_str, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "241e5ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/12 04:02:04 WARN DAGScheduler: Broadcasting large task binary with size 38.9 MiB\n",
      "25/04/12 04:02:45 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 235 (TID 446): Attempting to kill Python Worker\n",
      "25/04/12 04:02:45 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 235 (TID 446): Attempting to kill Python Worker\n",
      "25/04/12 04:03:01 WARN DAGScheduler: Broadcasting large task binary with size 38.9 MiB\n",
      "[Stage 236:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 294.903451\n",
      "R²: 0.416117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(val_data.map(lambda x: x.features))\n",
    "labels_and_preds = val_data.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "metrics = RegressionMetrics(labels_and_preds)\n",
    "print(f\"RMSE: {metrics.rootMeanSquaredError:.6f}\")\n",
    "print(f\"R²: {metrics.r2:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cdd094",
   "metadata": {},
   "source": [
    "### Run on test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_local = [\"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "\n",
    "test_rows = test_df.select(\"id\", *feature_cols_local).collect()\n",
    "\n",
    "results = []\n",
    "for row in test_rows:\n",
    "    id_val = row[\"id\"]\n",
    "    features = Vectors.dense([row[c] for c in feature_cols_local])\n",
    "    prediction = float(model.predict(features))\n",
    "    results.append((id_val, prediction))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"prediction\", DoubleType(), True)\n",
    "])\n",
    "predicted_df = spark.createDataFrame(results, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da45e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 10:47:39 WARN TaskSetManager: Stage 53 contains a task of very large size (14066 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "predicted_df.toPandas().to_csv(\"results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
