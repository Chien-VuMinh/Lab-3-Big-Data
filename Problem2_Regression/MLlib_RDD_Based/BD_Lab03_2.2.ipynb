{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9da7f8",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4961174",
   "metadata": {},
   "source": [
    "### Create session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e6df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/11 10:04:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLlib\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b7facd",
   "metadata": {},
   "source": [
    "### Prepare data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab8f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_path = \"train.csv\"\n",
    "test_path = \"test.csv\"\n",
    "train_df = spark.read.csv(train_path, header=True, inferSchema=True)\n",
    "test_df = spark.read.csv(test_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5262bef",
   "metadata": {},
   "source": [
    "#### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf47c4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "quantiles = train_df.approxQuantile(\"trip_duration\", [0.25, 0.75], 0.01)\n",
    "Q1, Q3 = quantiles\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "train_df = train_df.filter((col(\"trip_duration\") >= lower_bound) & (col(\"trip_duration\") <= upper_bound))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55036a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "\n",
    "def to_labeled_point(row):\n",
    "    features = [row[c] for c in feature_cols]\n",
    "    return LabeledPoint(row[\"trip_duration\"], Vectors.dense(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = train_df.rdd.map(to_labeled_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66771d0",
   "metadata": {},
   "source": [
    "### Split data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_rdd.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d5849",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2157050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 10:19:53 WARN DAGScheduler: Broadcasting large task binary with size 1496.8 KiB\n",
      "25/04/11 10:19:53 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/04/11 10:19:54 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/04/11 10:19:55 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "25/04/11 10:19:56 WARN DAGScheduler: Broadcasting large task binary with size 1069.0 KiB\n",
      "25/04/11 10:19:57 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n",
      "25/04/11 10:19:57 WARN DAGScheduler: Broadcasting large task binary with size 1379.4 KiB\n",
      "25/04/11 10:19:58 WARN DAGScheduler: Broadcasting large task binary with size 11.1 MiB\n",
      "25/04/11 10:19:59 WARN DAGScheduler: Broadcasting large task binary with size 1652.8 KiB\n",
      "25/04/11 10:20:00 WARN DAGScheduler: Broadcasting large task binary with size 14.6 MiB\n",
      "25/04/11 10:20:02 WARN DAGScheduler: Broadcasting large task binary with size 1889.6 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = DecisionTree.trainRegressor(\n",
    "    train_data,\n",
    "    categoricalFeaturesInfo={},\n",
    "    impurity=\"variance\",\n",
    "    maxDepth=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0acf4",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e5ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macos/Desktop/hcmus/big_data/Lab-3-Big-Data/venv/lib/python3.9/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "25/04/11 10:20:07 WARN DAGScheduler: Broadcasting large task binary with size 16.1 MiB\n",
      "25/04/11 10:20:47 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 50 (TID 95): Attempting to kill Python Worker\n",
      "25/04/11 10:20:47 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 50 (TID 95): Attempting to kill Python Worker\n",
      "25/04/11 10:20:47 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 50 (TID 95): Attempting to kill Python Worker\n",
      "25/04/11 10:20:55 WARN DAGScheduler: Broadcasting large task binary with size 16.1 MiB\n",
      "[Stage 51:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 289.026082\n",
      "R²: 0.367998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(val_data.map(lambda x: x.features))\n",
    "labels_and_preds = val_data.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "metrics = RegressionMetrics(labels_and_preds)\n",
    "print(f\"RMSE: {metrics.rootMeanSquaredError:.6f}\")\n",
    "print(f\"R²: {metrics.r2:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cdd094",
   "metadata": {},
   "source": [
    "### Run on test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022049f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_local = [\"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "\n",
    "test_rows = test_df.select(\"id\", *feature_cols_local).collect()\n",
    "\n",
    "results = []\n",
    "for row in test_rows:\n",
    "    id_val = row[\"id\"]\n",
    "    features = Vectors.dense([row[c] for c in feature_cols_local])\n",
    "    prediction = float(model.predict(features))\n",
    "    results.append((id_val, prediction))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"prediction\", DoubleType(), True)\n",
    "])\n",
    "predicted_df = spark.createDataFrame(results, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da45e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 10:47:39 WARN TaskSetManager: Stage 53 contains a task of very large size (14066 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "predicted_df.toPandas().to_csv(\"results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
