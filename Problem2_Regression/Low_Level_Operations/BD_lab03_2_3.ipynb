{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc16db5",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a2d8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343aa393",
   "metadata": {},
   "source": [
    "## Create spark contex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4598b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9d40a",
   "metadata": {},
   "source": [
    "## Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37e2ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"file:///home/cap/hadoop/hadoop-3.4.1/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe8c19b",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "Base on the analysed from structure API we only choose 5 features (\"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\") for building decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5d5b8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing values: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# remove header and read data\n",
    "header = raw_data.first()\n",
    "\n",
    "lst_header = header.split(',')\n",
    "dict_header = {}\n",
    "for i, head in enumerate(lst_header):\n",
    "    dict_header[head] = i\n",
    "dict_header\n",
    "choosen_header = [\"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "\n",
    "data = raw_data.filter(lambda line: line != header)\n",
    "parsedData = data.map(\\\n",
    "                lambda line: [x for x in line.split(\",\")])\n",
    "\n",
    "# check missing value\n",
    "def has_missing(row):\n",
    "    for item in row:\n",
    "        if item is None:\n",
    "            return True\n",
    "        if isinstance(item, str) and (item == \"\" or item.lower() in ['na', 'null', 'nan']):\n",
    "            return True\n",
    "    return False\n",
    "missing_rows = parsedData.filter(has_missing).count()\n",
    "print(f\"Rows with missing values: {missing_rows}\")\n",
    "\n",
    "# remove duplicate values\n",
    "rdd_data = parsedData.map(lambda cols: (tuple(cols[:-1]), cols[-1])).distinct()\n",
    "rdd_data = rdd_data.map(lambda x: list((list(*x[:-1],), x[-1])))\n",
    "\n",
    "\n",
    "# prepare data in proper format\n",
    "def prepare_features(fields):\n",
    "    features = [float(fields[0][dict_header[i]]) for i in choosen_header]\n",
    "    label = float(fields[-1]  )            \n",
    "    return (features, label)\n",
    "\n",
    "rdd_data = rdd_data.map(prepare_features).cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc214f9",
   "metadata": {},
   "source": [
    "All numeric values have been archived "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15c1f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 20:01:09 WARN BlockManager: Task 13 already completed, not releasing lock for rdd_8_0\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([1.0,\n",
       "   -73.9790267944336,\n",
       "   40.763938903808594,\n",
       "   -74.00533294677734,\n",
       "   40.710086822509766],\n",
       "  2124.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb3693",
   "metadata": {},
   "source": [
    "Remove outlier in `trip_duration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b01882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len: 1458644\n",
      "After remove outliers: 1384424\n",
      "Removed row: 74220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# percentile\n",
    "def calculate_quantiles(rdd, quantiles=[0.25, 0.75]):\n",
    "    durations = rdd.map(lambda x: x[1]).collect()  \n",
    "    durations_sorted = sorted(durations)\n",
    "    n = len(durations_sorted)\n",
    "    \n",
    "    if n == 0:\n",
    "        return [0.0] * len(quantiles)\n",
    "    \n",
    "\n",
    "    quantile_values = []\n",
    "    for q in quantiles:\n",
    "        index = int(n * q)\n",
    "        if index >= n:\n",
    "            index = n - 1\n",
    "        quantile_values.append(durations_sorted[index])\n",
    "    \n",
    "    return quantile_values\n",
    "\n",
    "# Q1  Q3\n",
    "Q1, Q3 = calculate_quantiles(rdd_data, [0.25, 0.75])\n",
    "\n",
    "# IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# bound\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# remove outliers\n",
    "clean_data = rdd_data.filter(lambda x: lower_bound <= x[1] <= upper_bound).cache()\n",
    "\n",
    "\n",
    "original_count = rdd_data.count()\n",
    "clean_count = clean_data.count()\n",
    "print(f\"Original len: {original_count}\")\n",
    "print(f\"After remove outliers: {clean_count}\")\n",
    "print(f\"Removed row: {original_count - clean_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f350fddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# sorted and devided the feature into n equal parts to get threshold\n",
    "def get_thresholds(rdd, feature_idx, num_thresholds=5):\n",
    "    values = rdd.map(lambda x: x[0][feature_idx]).collect()\n",
    "    sorted_values = sorted(values)\n",
    "    step = len(sorted_values) // (num_thresholds + 1)\n",
    "    return [sorted_values[i * step] for i in range(1, num_thresholds + 1)]\n",
    "\n",
    "\n",
    "thresholds = {\n",
    "    0: [1, 2, 3, 4, 5],  # passenger_count\n",
    "    1: get_thresholds(clean_data, 1),  # pickup_longitude\n",
    "    2: get_thresholds(clean_data, 2),  # pickup_latitude\n",
    "    3: get_thresholds(clean_data, 3),  # dropoff_longitude\n",
    "    4: get_thresholds(clean_data, 4)   # dropoff_latitude\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def calculate_variance(rdd):\n",
    "    count = rdd.count()\n",
    "    if count < 10:  # Ngăn nút quá nhỏ\n",
    "        return 0.0, 0.0, count\n",
    "    sum_duration = rdd.map(lambda x: x[1]).reduce(lambda x, y: x + y)\n",
    "    mean = sum_duration / count\n",
    "    variance = rdd.map(lambda x: (x[1] - mean)**2).reduce(lambda x, y: x + y) / count\n",
    "    return variance, mean, count\n",
    "\n",
    "\n",
    "\n",
    "# funct to find the best split\n",
    "def find_best_split(rdd, features_idx, thresholds):\n",
    "    best_split = None\n",
    "    best_variance = float(\"inf\")\n",
    "    best_left_rdd = None\n",
    "    best_right_rdd = None\n",
    "    best_mean_left = None\n",
    "    best_mean_right = None\n",
    "    best_variance_reduction = 0.0\n",
    "\n",
    "    parent_var, _, parent_count = calculate_variance(rdd)\n",
    "\n",
    "    for idx in features_idx:\n",
    "        for threshold in thresholds[idx]:\n",
    "            left_rdd = rdd.filter(lambda x: x[0][idx] <= threshold)\n",
    "            right_rdd = rdd.filter(lambda x: x[0][idx] > threshold)\n",
    "\n",
    "            left_var, left_mean, left_count = calculate_variance(left_rdd)\n",
    "            right_var, right_mean, right_count = calculate_variance(right_rdd)\n",
    "\n",
    "            if left_count < 10 or right_count < 10:\n",
    "                continue\n",
    "\n",
    "            total_count = left_count + right_count\n",
    "            total_variance = (left_count / total_count) * left_var + (right_count / total_count) * right_var\n",
    "            variance_reduction = parent_var - total_variance\n",
    "\n",
    "            if total_variance < best_variance:\n",
    "                best_variance = total_variance\n",
    "                best_split = (idx, threshold)\n",
    "                best_left_rdd = left_rdd\n",
    "                best_right_rdd = right_rdd\n",
    "                best_mean_left = left_mean\n",
    "                best_mean_right = right_mean\n",
    "                best_variance_reduction = variance_reduction\n",
    "\n",
    "    return best_split, best_left_rdd, best_right_rdd, best_mean_left, best_mean_right, best_variance_reduction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_tree(rdd, depth, max_depth, features_idx, thresholds, feature_counts=None):\n",
    "    if feature_counts is None:\n",
    "        feature_counts = {i: 0.0 for i in features_idx}\n",
    "    \n",
    "    if depth >= max_depth or rdd.count() < 10:\n",
    "        _, mean, count = calculate_variance(rdd)\n",
    "        return {\"leaf\": True, \"value\": mean, \"count\": count}, feature_counts\n",
    "\n",
    "    split, left_rdd, right_rdd, mean_left, mean_right, variance_reduction = find_best_split(rdd, features_idx, thresholds)\n",
    "    if split is None:\n",
    "        _, mean, count = calculate_variance(rdd)\n",
    "        return {\"leaf\": True, \"value\": mean, \"count\": count}, feature_counts\n",
    "\n",
    "    feature_idx, threshold = split\n",
    "    total_count = rdd.count()\n",
    "    feature_counts[feature_idx] += variance_reduction * total_count  # Cộng dồn tầm quan trọng\n",
    "\n",
    "    left_node, feature_counts = build_tree(left_rdd, depth + 1, max_depth, features_idx, thresholds, feature_counts)\n",
    "    right_node, feature_counts = build_tree(right_rdd, depth + 1, max_depth, features_idx, thresholds, feature_counts)\n",
    "\n",
    "    return {\n",
    "        \"leaf\": False,\n",
    "        \"feature_idx\": feature_idx,\n",
    "        \"threshold\": threshold,\n",
    "        \"left\": left_node,\n",
    "        \"right\": right_node,\n",
    "        \"count\": total_count\n",
    "    }, feature_counts\n",
    "\n",
    "\n",
    "\n",
    "def print_tree(node, depth=0, feature_names=None):\n",
    "    indent = \"  \" * depth\n",
    "    if node[\"leaf\"]:\n",
    "        print(f\"{indent}Leaf: predict={node['value']:.2f}, samples={node['count']}\")\n",
    "        return\n",
    "    feature = feature_names[node[\"feature_idx\"]] if feature_names else f\"feature_{node['feature_idx']}\"\n",
    "    print(f\"{indent}Node: {feature} <= {node['threshold']:.4f}, samples={node['count']}\")\n",
    "    print_tree(node[\"left\"], depth + 1, feature_names)\n",
    "    print_tree(node[\"right\"], depth + 1, feature_names)\n",
    "\n",
    "\n",
    "\n",
    "def predict(tree, features):\n",
    "    if tree[\"leaf\"]:\n",
    "        return tree[\"value\"]\n",
    "    feature_idx = tree[\"feature_idx\"]\n",
    "    threshold = tree[\"threshold\"]\n",
    "    if features[feature_idx] <= threshold:\n",
    "        return predict(tree[\"left\"], features)\n",
    "    else:\n",
    "        return predict(tree[\"right\"], features)\n",
    "    \n",
    "\n",
    "\n",
    "# Hàm tính RMSE và R²\n",
    "def calculate_metrics(rdd, tree):\n",
    "    predictions = rdd.map(lambda x: (predict(tree, x[0]), x[1]))\n",
    "    n = predictions.count()\n",
    "    if n == 0:\n",
    "        return float(\"inf\"), 0.0\n",
    "\n",
    "    # Tính RMSE\n",
    "    squared_errors = predictions.map(lambda x: (x[0] - x[1])**2)\n",
    "    mse = squared_errors.reduce(lambda x, y: x + y) / n\n",
    "    rmse = math.sqrt(mse)\n",
    "\n",
    "    # Tính R²\n",
    "    mean_y = predictions.map(lambda x: x[1]).reduce(lambda x, y: x + y) / n\n",
    "    ss_tot = predictions.map(lambda x: (x[1] - mean_y)**2).reduce(lambda x, y: x + y)\n",
    "    ss_res = squared_errors.reduce(lambda x, y: x + y)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
    "\n",
    "    return rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96f4c058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cấu trúc cây quyết định:\n",
      "Node: dropoff_latitude <= 40.7272, samples=1384424\n",
      "  Node: dropoff_latitude <= 40.7272, samples=1153720\n",
      "    Node: dropoff_latitude <= 40.7272, samples=1153720\n",
      "      Node: dropoff_latitude <= 40.7272, samples=1153720\n",
      "        Node: dropoff_latitude <= 40.7272, samples=1153720\n",
      "          Leaf: predict=739.25, samples=1153720\n",
      "          Leaf: predict=0.00, samples=0\n",
      "        Leaf: predict=0.00, samples=0\n",
      "      Leaf: predict=0.00, samples=0\n",
      "    Leaf: predict=0.00, samples=0\n",
      "  Node: pickup_latitude <= 40.7632, samples=230704\n",
      "    Leaf: predict=0.00, samples=0\n",
      "    Node: pickup_latitude <= 40.7632, samples=230704\n",
      "      Leaf: predict=0.00, samples=0\n",
      "      Node: pickup_latitude <= 40.7632, samples=230704\n",
      "        Leaf: predict=0.00, samples=0\n",
      "        Node: pickup_latitude <= 40.7632, samples=230704\n",
      "          Leaf: predict=0.00, samples=0\n",
      "          Leaf: predict=693.96, samples=230704\n",
      "\n",
      "Feature Importances:\n",
      "passenger_count: 0.0000\n",
      "pickup_longitude: 0.0000\n",
      "pickup_latitude: 0.5774\n",
      "dropoff_longitude: 0.0000\n",
      "dropoff_latitude: 0.4226\n",
      "\n",
      "Train RMSE: 679.00\n",
      "Train R²: -1.2960\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng cây\n",
    "features_idx = [0, 1, 2, 3, 4]\n",
    "max_depth = 5\n",
    "tree, feature_counts = build_tree(clean_data, 0, max_depth, features_idx, thresholds)\n",
    "\n",
    "# In cây\n",
    "print(\"Cấu trúc cây quyết định:\")\n",
    "print_tree(tree, feature_names=choosen_header)\n",
    "\n",
    "# Tính Feature Importances\n",
    "total_importance = sum(feature_counts.values())\n",
    "feature_importances = {choosen_header[i]: (count / total_importance if total_importance > 0 else 0.0)\n",
    "                      for i, count in feature_counts.items()}\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in feature_importances.items():\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Tính RMSE và R² trên tập train\n",
    "train_rmse, train_r2 = calculate_metrics(clean_data, tree)\n",
    "print(f\"\\nTrain RMSE: {train_rmse:.2f}\")\n",
    "print(f\"Train R²: {train_r2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872441dd",
   "metadata": {},
   "source": [
    "## Preprocess test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8386b661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2781:==================>                                     (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing values: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_test = sc.textFile(\"file:///home/cap/hadoop/hadoop-3.4.1/test.csv\")\n",
    "\n",
    "# remove header and read data\n",
    "header = raw_test.first()\n",
    "\n",
    "lst_header = header.split(',')\n",
    "dict_header = {}\n",
    "for i, head in enumerate(lst_header):\n",
    "    dict_header[head] = i\n",
    "dict_header\n",
    "choosen_header = [\"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "\n",
    "data = raw_test.filter(lambda line: line != header)\n",
    "parsedData = data.map(\\\n",
    "                lambda line: [x for x in line.split(\",\")])\n",
    "\n",
    "# check missing value\n",
    "def has_missing(row):\n",
    "    for item in row:\n",
    "        if item is None:\n",
    "            return True\n",
    "        if isinstance(item, str) and (item == \"\" or item.lower() in ['na', 'null', 'nan']):\n",
    "            return True\n",
    "    return False\n",
    "missing_rows = parsedData.filter(has_missing).count()\n",
    "print(f\"Rows with missing values: {missing_rows}\")\n",
    "\n",
    "# remove duplicate values\n",
    "rdd_data = parsedData.map(lambda cols: (tuple(cols[:-1]))).distinct()\n",
    "rdd_data = rdd_data.map(lambda x: (list(x)))\n",
    "\n",
    "\n",
    "# prepare data in proper format\n",
    "def prepare_features(fields):\n",
    "    features = [float(fields[dict_header[i]]) for i in choosen_header]      \n",
    "    return (features)\n",
    "\n",
    "rdd_data = rdd_data.map(prepare_features).cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc57ad",
   "metadata": {},
   "source": [
    "## Illustrate low-level implementation.\n",
    "Using 2 row from test set to implement and run code to check the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dcb121",
   "metadata": {},
   "source": [
    "Row 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0c8d120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd: [1.0, -73.9974365234375, 40.73758316040039, -73.98616027832031, 40.729522705078125], predict: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 21:32:20 WARN BlockManager: Task 8314 already completed, not releasing lock for rdd_1423_0\n",
      "25/04/11 21:32:20 WARN BlockManager: Task 8315 already completed, not releasing lock for rdd_1423_0\n"
     ]
    }
   ],
   "source": [
    "print(f'rdd: {rdd_data.take(1)[0]}, predict: {predict(tree, rdd_data.take(1)[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fcc953",
   "metadata": {},
   "source": [
    "- Here dropoff_latitude = 40.7295 > 40.7272 so it will go to the right node\n",
    "- Next pickup_latitude = 40.73758 <= 40.7632 so it will go to the left node\n",
    "- The left node is a leaf so predict value is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f764c",
   "metadata": {},
   "source": [
    "Row 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa507b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd: [1.0, -73.99176788330078, 40.764888763427734, -73.9789810180664, 40.7445182800293], predict: 693.9621549691379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 22:24:38 WARN BlockManager: Task 8325 already completed, not releasing lock for rdd_1423_0\n"
     ]
    }
   ],
   "source": [
    "x = rdd_data.take(53)[-1]\n",
    "print(f'rdd: {x}, predict: {predict(tree, x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce3e9b0",
   "metadata": {},
   "source": [
    "- Here dropoff_latitude = 40.7445 > 40.7272 so it will go to the right node\n",
    "- Next pickup_latitude = 40.76488 <= 40.7632 so it will go to the right node\n",
    "- The pickup_latitude = 40.76488 <= 40.7632 so it will go to the right node\n",
    "- The pickup_latitude = 40.76488 <= 40.7632 so it will go to the right node\n",
    "- The pickup_latitude = 40.76488 <= 40.7632 so it will go to the right node\n",
    "- The predict value is 693.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c65ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đóng Spark\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
